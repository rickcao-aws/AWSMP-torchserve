{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please update libraries and SDKs before we start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip -q install sagemaker awscli boto3 --upgrade "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Model Listing with TorchServe on AWS Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook, we will use `TorchServe` and `Sagemaker SDK` to create a Docker image as a base, and re-use the docker image to list different model package products on AWS Marketplace with different `model data`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the TorchServe repository and install torch-model-archiver\n",
    "\n",
    "We'll use `torch-model-archiver` to create a model archive file (.mar). We'll use the .mar model archive file as the `model data` in listing different Pytorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/serve.git\n",
    "!pip install serve/model-archiver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a boto3 session and get specify a role with SageMaker access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, time, json\n",
    "sess    = boto3.Session()\n",
    "sm      = sess.client('sagemaker')\n",
    "region  = sess.region_name\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Amazon ECR registry through AWS CLI\n",
    "Create a new docker container registry for your torchserve container images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_name = 'torchserve-base'\n",
    "!aws ecr create-repository --repository-name {registry_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a TorchServe Docker image and push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_label = 'v1'\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{registry_name}:{image_label}'\n",
    "\n",
    "!docker build -t {registry_name}:{image_label} .\n",
    "!$(aws ecr get-login --no-include-email --region {region})\n",
    "!docker tag {registry_name}:{image_label} {image}\n",
    "!docker push {image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Remember to scan your docker image in Amazon ECR after you pushed the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./img/docker_image_scan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a PyTorch model and create a TorchServe archive\n",
    "Let's start with Densenet-161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://download.pytorch.org/models/densenet161-8d451a50.pth\n",
    "    \n",
    "model_file_name = 'densenet161'\n",
    "\n",
    "!torch-model-archiver --model-name {model_file_name} \\\n",
    "--version 1.0 --model-file serve/examples/image_classifier/densenet_161/model.py \\\n",
    "--serialized-file densenet161-8d451a50.pth \\\n",
    "--extra-files serve/examples/image_classifier/index_to_name.json \\\n",
    "--handler image_classifier\n",
    "\n",
    "!ls *.mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: You can also replace with your own `pth` model file and create the `mar` file with `torch-model-archiver`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the generated densenet161.mar archive file to Amazon S3\n",
    "Create a compressed tar.gz file from the densenet161.mar file since Amazon SageMaker expects that models are in a tar.gz file. \n",
    "Uploads the model to your default Amazon SageMaker S3 bucket under the models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'torchserve'\n",
    "\n",
    "!tar cvfz {model_file_name}.tar.gz densenet161.mar\n",
    "!aws s3 cp {model_file_name}.tar.gz s3://{bucket_name}/{prefix}/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy endpoint and make prediction using Amazon SageMaker SDK, to make sure torchserve is actually working as the base image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "model_data = f's3://{bucket_name}/{prefix}/models/{model_file_name}.tar.gz'\n",
    "sm_model_name = 'torchserve-densenet161'\n",
    "\n",
    "torchserve_model = Model(model_data = model_data, \n",
    "                         image_uri = image,\n",
    "                         role  = role,\n",
    "                         predictor_cls=Predictor,\n",
    "                         name  = sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'torchserve-endpoint-' + sm_model_name + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "predictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n",
    "                                    initial_instance_count=1,\n",
    "                                    endpoint_name = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the TorchServe hosted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://s3.amazonaws.com/model-server/inputs/kitten.jpg    \n",
    "file_name = 'kitten.jpg'\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = payload\n",
    "    \n",
    "response = predictor.predict(data=payload)\n",
    "print(*json.loads(response), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing the Pytorch Model on Marketplace\n",
    "In this section, we will work on listing two models with the same TorchServe base image. We only need to change the `model data` (the `mar` file) when creating another model package.\n",
    "\n",
    "Let's first strt with the downloaded `Densenet-161`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = 'torchserve-densenet161'\n",
    "batch_inference_input_prefix = \"batch-inference-input-data\"\n",
    "TRANSFORM_WORKDIR = \"transform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we create the model package, Let's first test the batch transform on our side\n",
    "First create the folder for transform input and download several images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# mkdir transform\n",
    "cd transform\n",
    "wget https://s3.amazonaws.com/model-server/inputs/kitten.jpg\n",
    "wget https://s3.amazonaws.com/model-server/inputs/flower.jpg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_input = sagemaker_session.upload_data(TRANSFORM_WORKDIR, key_prefix=batch_inference_input_prefix)\n",
    "print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = sagemaker.transformer.Transformer(model_name=sm_model_name, instance_count=1, instance_type='ml.m4.xlarge',\n",
    "                            strategy=None, assemble_with=None, output_path=None, sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(transform_input, content_type='image/jpeg')\n",
    "transformer.wait()\n",
    "\n",
    "print(\"Batch Transform output saved to \" + transformer.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratualations! Batch transform succeed on our side!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference_specification import InferenceSpecification\n",
    "import json\n",
    "\n",
    "modelpackage_inference_specification = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=image,\n",
    "    supports_gpu=True,\n",
    "    supported_content_types=[\"image/jpeg\", \"image/png\"],\n",
    "    supported_mime_types=[\"application/json\"])\n",
    "\n",
    "# Specify the model data resulting from the previously completed training job\n",
    "modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]= model_data\n",
    "print(json.dumps(modelpackage_inference_specification, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modelpackage_validation_specification import ModelPackageValidationSpecification\n",
    "import time\n",
    "\n",
    "modelpackage_validation_specification = ModelPackageValidationSpecification().get_validation_specification_dict(\n",
    "    validation_role = role,\n",
    "    batch_transform_input = transform_input,\n",
    "    input_content_type = \"image/jpeg\",\n",
    "    output_content_type = \"application/json\",\n",
    "    instance_type = \"ml.c4.xlarge\",\n",
    "    output_s3_location = 's3://{}/{}'.format(sagemaker_session.default_bucket(), \"/batch-inference-output-data\"))\n",
    "\n",
    "print(json.dumps(modelpackage_validation_specification, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_name = sm_model_name + \"-\" + str(round(time.time()))\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageName\" : model_package_name,\n",
    "    \"ModelPackageDescription\" : \"Model of pre-trained DenseNet161\",\n",
    "    \"CertifyForMarketplace\" : True\n",
    "}\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "create_model_package_input_dict.update(modelpackage_validation_specification)\n",
    "print(json.dumps(create_model_package_input_dict, indent=4, sort_keys=True))\n",
    "\n",
    "sm.create_model_package(**create_model_package_input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = sm.describe_model_package(ModelPackageName=model_package_name)\n",
    "    status = response[\"ModelPackageStatus\"]\n",
    "    print (status)\n",
    "    if (status == \"Completed\" or status == \"Failed\"):\n",
    "        print (response[\"ModelPackageStatusDetails\"])\n",
    "        break\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use another model data to list another model product\n",
    "Let's try with `Vgg-11` this time\n",
    "\n",
    "You can also replace it with your own `pth` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://download.pytorch.org/models/vgg11-bbd30ac9.pth\n",
    "    \n",
    "model_file_name_vgg11 = 'vgg11'\n",
    "\n",
    "!torch-model-archiver --model-name {model_file_name_vgg11} \\\n",
    "--version 1.0 --model-file serve/examples/image_classifier/vgg_11/model.py \\\n",
    "--serialized-file vgg11-bbd30ac9.pth \\\n",
    "--extra-files serve/examples/image_classifier/index_to_name.json \\\n",
    "--handler image_classifier\n",
    "\n",
    "!ls *.mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'torchserve'\n",
    "\n",
    "!tar cvfz {model_file_name_vgg11}.tar.gz vgg11.mar\n",
    "!aws s3 cp {model_file_name_vgg11}.tar.gz s3://{bucket_name}/{prefix}/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = f's3://{bucket_name}/{prefix}/models/{model_file_name_vgg11}.tar.gz'\n",
    "sm_model_name_vgg11 = 'torchserve-vgg11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpackage_inference_specification = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=image,\n",
    "    supports_gpu=True,\n",
    "    supported_content_types=[\"image/jpeg\", \"image/png\"],\n",
    "    supported_mime_types=[\"application/json\"])\n",
    "\n",
    "# Specify the model data resulting from the previously completed training job\n",
    "modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]= model_data\n",
    "print(json.dumps(modelpackage_inference_specification, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "modelpackage_validation_specification = ModelPackageValidationSpecification().get_validation_specification_dict(\n",
    "    validation_role = role,\n",
    "    batch_transform_input = transform_input,\n",
    "    input_content_type = \"image/jpeg\",\n",
    "    output_content_type = \"application/json\",\n",
    "    instance_type = \"ml.c4.xlarge\",\n",
    "    output_s3_location = 's3://{}/{}'.format(sagemaker_session.default_bucket(), \"/batch-inference-output-data\"))\n",
    "\n",
    "print(json.dumps(modelpackage_validation_specification, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_name = sm_model_name_vgg11 + \"-\" + str(round(time.time()))\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageName\" : model_package_name,\n",
    "    \"ModelPackageDescription\" : \"Model of pre-trained VGG11\",\n",
    "    \"CertifyForMarketplace\" : True\n",
    "}\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "create_model_package_input_dict.update(modelpackage_validation_specification)\n",
    "print(json.dumps(create_model_package_input_dict, indent=4, sort_keys=True))\n",
    "\n",
    "sm.create_model_package(**create_model_package_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have succeeded creating two model packages for ML MarketPlace listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./img/listing_on_marketplace.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
